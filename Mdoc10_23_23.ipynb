{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34bde6d8",
   "metadata": {},
   "source": [
    "# Master learning document + random ideas and goals\n",
    "\n",
    "------------\n",
    "\n",
    "------------\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7a7fb",
   "metadata": {},
   "source": [
    "# Future\n",
    "\n",
    "## model ideas:\n",
    "\n",
    "use evolutionary algoirthms for NN architecture and hyper params?\n",
    "\n",
    "## projects:\n",
    "\n",
    "create classifation app, use tensorboard to monitor background and get it to run live updates\n",
    "\n",
    "autopodcast, given a webpage creates a podcast.\n",
    "\n",
    "\n",
    "## RL ideas:\n",
    "\n",
    "**the ideas are the easy part**\n",
    "\n",
    "external \"conciousness\" model produce correct behavior? \n",
    "\n",
    "GAN's for world models. no rent free world representations.\n",
    "\n",
    "how to get two agents to communicate world states and take actions from that update world state to cut back on trail and error.\n",
    "\n",
    "genetic algorithms using two parents + variation?\n",
    "\n",
    "since deep Q tries to find a function approximation for a policy, have a metafunction use an attention mechanism to try and match what environment it is in and what approximation function should be used THis way it can learn to do many things in many different domains.+ threshold for higher RL overview.\n",
    "\n",
    "in thinking fast and slow, is the \"thinking fast\" kind of akin to letting the model run and then updating the \"thinking slow\" model later? i dont think it applies but I do think the brain does something kind of like this.\n",
    "\n",
    "stateful RL needs the state to be in the replay buffer so when the experience is sampled the policy is set to the policies state when the experience happens. closest I can think of is memories evoking emotions. I realize I'm pigeon holing but it is interesting.\n",
    "\n",
    "finding a way to add an autoencoder to RL would probably help sample efficiency\n",
    "\n",
    "understand how to mask environments.\n",
    "\n",
    "if Deep Q-Networks are finding a function for approximate Q values I'm assuming it is pretty domain specific, it doesn't seem you can throw that function at another domain and espect it work without recalibrating th function to deal with the domain shift. These functions also need a metafunction that creates functions to deal with domain specific problems.\n",
    "\n",
    "humans seem to do a dual exploration + rewards based learning depending on the environment context, maybe good to have param to increase what rewards one prioritizes. Or consider them two different agents under a master agent who can domain switch. like as a kid I liked to play and explore, but I ended up listening to parents because the rewards (or punishments for not listening) were greater than exploring, of course there were times I prioritized exploration to the anger of my parents and loss of future happiness (rewards?) there.\n",
    "\n",
    "it seems RL desperately needs a way to split domain contexts and switch between them.\n",
    "\n",
    "what is a \"good policy\"? a policy is just a function for the environment right? or is it something more? DQN is attemption to generate a function that can output an appoximate Q-value given a state in that environment. and usually based on that an action is taken. can all our brain does be reduce down to lossy function modeling and action execution (with a lot of complexity)? ignoring the philosophical, if all DQN's do is make a function for estimation based on an environment (domain) then finding research on domain joining and connecting it together with deepl RL should give a very versitile agent. \n",
    "\n",
    "and if it is a an action function for an environment that DQN's are using then somehow being able to pre-model that function or finding a way to ad skip connections could greatly increase sample efficiency. We humans I imagine have something like this, everything we learn when we're grown seems like fine-tuning to me, but domain specific sub RL agent creation. I think autoencoders are the bridge there somehow, to pass general domain information onto a subdomain then fine-tune from there within the same general \"agent\". if we can add on parralelism we get a bot that can walk and chew gum at the same time. then just make a threshold where if there is a prediction error the main RL head focuses attention on it and resolves it by uppring the learning rate or something.\n",
    "\n",
    "resolving this seems to be what I need to work on, and I need to create a robot with enough versitile actions available e.g a basic arm and wheels to do it. I wonder how feasably I think this be when I actually really dive deep into RL, regardless it will need to happen.\n",
    "\n",
    "**the versitile robot**: like a simple bot to get drinks, learn peoples names, learn anything really. it would need a master RL agent (combined with attention mechanisms?) to identify what domain it is in and either use an already trained sub RL agent for that domain or generate a new one, the generated new one would idealy be trained by RLHF. if there is a prediction error it trys to resolve it internally or then gets help.\n",
    "\n",
    "\n",
    "\n",
    "## Needs connecting\n",
    "a lot of boosting techniques make things that look at relationships that move together, see local response normalization and SE blocks in CNN's\n",
    "\n",
    "neighboring values are used in a lot of places for estimating a value, neighbors count for a lot.\n",
    "\n",
    "seems like there are two things in deep learning, generating functions and generating probability distributions, I'm sure there has been a way to combine these, or they are already combined. Maybe I'll run across a paper like that.\n",
    "\n",
    "## Plan after the book\n",
    "start reading through papers field by field and getting better, until you have a solid idea of what SOTA is in a certain subfield, what they are working on, and where they might be headed., write a report on it for the blog. Do this every year so you don't fall behind.\n",
    "\n",
    "book -> stats -> datascience -> fourier / complex analysis + bayesian + monte carlo -> embeded systems and edge computing -> ???\n",
    "\n",
    "\n",
    "maybe add: \n",
    " \n",
    "dynamics (?) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Random observations and ad hoc theorys\n",
    "\n",
    "a lot of modeling seems to have a greedy parameter that starts high and ends low as time goes on. like learning rate, make big jumps in gradient desecent then lower it as you get to the optimum. Children seem to learn better than adults, is a similar procress in humans?\n",
    "\n",
    "humans also seem to have a local response normalization, how many siblings go into the same field as each other in the long run? life seems to like diverse output.\n",
    "\n",
    "&humans seem to do reassignment of credit for bad actions in therapy, recontextualizing the \"supposed\" bad actions to the (maybe) \"real\" bad actions. also short vs long term rewards are given sometimes through parents or teachers (good things come to those who wait) or the enviroment (touching a cat in the wrong place it scratches you), humans seem to usually give sparse long term rewards and the environment usually seems to give short term rewards (with excepts of course)\n",
    "\n",
    "I wonder if there is transfer learning for RL agents?\n",
    "\n",
    "are there meta autoencoders that try to set up autoencoders for variable input data? \n",
    "\n",
    "It seems letting the model learn the important features is always better than hand crafted features\n",
    "\n",
    "socio/psychopathy = dicount factor of 0? only present matters.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Areas to keep an eye on\n",
    "\n",
    "### transformers, especially multimodel\n",
    "\n",
    "### RL \n",
    "\n",
    "### Control theory\n",
    "\n",
    "### Genetic algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea3fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3e5ac73",
   "metadata": {},
   "source": [
    "# Need to understand deeper ML\n",
    "TPU's\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning\n",
    "\n",
    "XGBOOST\n",
    "\n",
    "Catboost:\n",
    "https://catboost.ai/en/docs/\n",
    "You can integrate neural net outputs into gradient boosting models (yandex catboost talks about this, figure out how)\n",
    "\n",
    " \n",
    "Decision trees\n",
    "\n",
    "autodiff\n",
    "\n",
    "bayesian optimization:\n",
    "https://distill.pub/2020/bayesian-optimization/\n",
    "\n",
    "https://www.youtube.com/@machinelearningmastery/videos\n",
    "\n",
    "https://www.microsoft.com/en-us/research/video/introduction-to-bayesian-optimization/\n",
    "\n",
    "\n",
    "Featurewiz:\n",
    "\n",
    "https://github.com/AutoViML/featurewiz\n",
    "\n",
    "https://towardsdatascience.com/featurewiz-fast-way-to-select-the-best-features-in-a-data-9c861178602e\n",
    "\n",
    "https://arxiv.org/abs/2207.01848\n",
    "\n",
    "SELU:\n",
    "https://arxiv.org/abs/1706.02515\n",
    "\n",
    "---\n",
    "\n",
    "# Need to understand deeper Prob/stats\n",
    "\n",
    "Exponental moving average\n",
    "\n",
    "bayesian inference.\n",
    "\n",
    "Fixup initalization\n",
    "\n",
    "Deep Gaussian Process.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Concentration_inequality for model testing?\n",
    "\n",
    "---\n",
    "\n",
    "# Deeper understanding DL\n",
    "\n",
    "onecycle scheduling.\n",
    "https://github.com/ageron/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb at the bottom\n",
    "\n",
    "Autograph and tracing in TF / dynamic graphing in pytorch.\n",
    "\n",
    "## CNN's\n",
    "bottleneck layers and depthwise seperable convolution layer(s)\n",
    "\n",
    "Fully convolutional networks.\n",
    "\n",
    "semantic / instance ssegmantation\n",
    "\n",
    "mask / fast CNN papers\n",
    "\n",
    "adversarial learning\n",
    "\n",
    "single shot learning\n",
    "\n",
    "capsule networks?\n",
    "\n",
    "ResNeXt \n",
    "\n",
    "DenseNet\n",
    "\n",
    "MobileNet\n",
    "\n",
    "CSPNet \n",
    "\n",
    "EfficientNet\n",
    "\n",
    "## Transformers:\n",
    "temporal fusion tranformers:\n",
    "\n",
    "https://arxiv.org/abs/1912.09363\n",
    "\n",
    "tabfn:\n",
    "https://www.kaggle.com/code/abhinavmangalore/icr-tabpfn-and-xgboost-ensemble\n",
    "\n",
    "https://arxiv.org/abs/2207.01848\n",
    "\n",
    "https://www.kaggle.com/code/muelsamu/simple-tabpfn-approach-for-score-of-15-in-1-min\n",
    "\n",
    "vision transformers (ViTs) \n",
    "\n",
    "data-efficient image transformers (DeiTs): Perceiver, and DINO, \n",
    "\n",
    "large multimodal models: CLIP, DALLÂ·E, Flamingo, and GATO.\n",
    "\n",
    "## Resnets:\n",
    "how to make most networks a residual network\n",
    "\n",
    "## RNN's:\n",
    "\n",
    "back prob through time calculations. \n",
    "\n",
    "encoder decoder networks: https://arxiv.org/abs/1409.3215\n",
    "\n",
    "transformer dode\n",
    "\n",
    "ARMA model and its variants.\n",
    "\n",
    "Switch Transformers: DistilBERT, T5, and PaLM\n",
    "\n",
    "## GANs:\n",
    "\n",
    "stylenet\n",
    "\n",
    "## Generative models:\n",
    "\n",
    "diffusion models\n",
    "\n",
    "denoising diffusion probabilistic model (DDPM)\n",
    "\n",
    "## RL:\n",
    "Temporal Distance learning\n",
    "\n",
    "pretty much every paper from the book.\n",
    "\n",
    "Gridworld.\n",
    "\n",
    "most used evironment respoitory (OpenAI gym?)\n",
    "\n",
    "\n",
    "\n",
    "RNN agents for state machines?\n",
    "\n",
    "## Visualizations:\n",
    " \n",
    "tensorboard\n",
    "\n",
    "wandb \n",
    "\n",
    "compare TFlite model to og model with Netron graph visualization tool to see optimization\n",
    "\n",
    "## Other\n",
    "\n",
    "federated learning.\n",
    " \n",
    "colab for easy test sharing.\n",
    "\n",
    "---\n",
    "\n",
    "# Deeper python functionality\n",
    "\n",
    "https://builtin.com/software-engineering-perspectives/what-is-with-statement-python (done) 9/10/23\n",
    "\n",
    "\n",
    "https://builtin.com/software-engineering-perspectives/python-symbol (done) 9/10/23\n",
    "\n",
    "\n",
    "-Decorators (done) 9/10/23\n",
    "\n",
    "-linked lists\n",
    "\n",
    "-hashes \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Data\n",
    "\n",
    "Binary files?\n",
    "\n",
    "Pytorch protobuffs? \n",
    "\n",
    "Apache Beam or Spark for large datasets \n",
    "\n",
    "FlatBuffers?\n",
    "\n",
    "----\n",
    "\n",
    "# Job stuff\n",
    "Agile System Development Lifecycle\n",
    "\n",
    "PEP8 for python\n",
    "\n",
    "autolinters\n",
    "\n",
    "Find mentor\n",
    "\n",
    "----\n",
    "\n",
    "# Deploying and serving\n",
    "\n",
    "model versioning\n",
    "\n",
    "model servicing platforms (flask?), make sure it allows batching queries.\n",
    "\n",
    "hardware infrastructure platforms.\n",
    "\n",
    "how metagraphs (if they are called that), are used in pytorch\n",
    "\n",
    "read the sidebar on reddit self hosted\n",
    "\n",
    "use gRPC API for model querying since RESTAPI uses JSON which is horribly inefficient. \n",
    "\n",
    "if expecting many QPS deploy serving containers on multiple servers and load-balence the queries. requries deploying and maintaining many serving containers across servers. Kubernetes is an open source system for simplifying container organization across servers.\n",
    "\n",
    "find a good platform-as-a-service (PaaS) so you don't have to deal with hardware infrastructure. can use kubernetes to handle virtual machines too. look for ones with TPU's?\n",
    "\n",
    "remember on platforms to be careful with costs and to not run things if you don't need too. SET AN ALARM so you can turn projects off when you want too. Make sure to use automatic scaling if possible.\n",
    "\n",
    "look for API client librarys for writing scripts to query the service if using a platform.\n",
    "\n",
    "TFlite for mobile / embedded deployment\n",
    "\n",
    "quantization for smaler / faster models. (look into tinyML)\n",
    "\n",
    "----\n",
    "# Hardware\n",
    "\n",
    "check out Tim Dettmers blog on GPU's\n",
    "\n",
    "check to see what GPU's Pytorch / TF supports (needs CUDA compute capability), check the docs\n",
    "\n",
    "look for a docker image with the correct GPU libraries.\n",
    "\n",
    "# Pytorch Coding\n",
    "\n",
    "## Pretraining:\n",
    "find where to get pretrained models and how to do transfer learning.\n",
    "\n",
    "find how to find where the input requirements for pretrained models are.\n",
    "\n",
    "specific part of pretrained models, like embeddings (find equivelent to TF Hub)\n",
    "\n",
    "## CNN's\n",
    "\n",
    "recreate pg 454 of da book.\n",
    "\n",
    "batch and channel strides impementations.\n",
    "\n",
    "crop and resize image function?\n",
    "\n",
    "transpose convl layer.\n",
    "\n",
    "1d convl layers, 3d convl layers\n",
    "\n",
    "dilation_rate for a a-trous convl layer, allowing a larger receptive field.\n",
    "\n",
    "depthwise convl layer, applying every filter to every individual input chennel independently.\n",
    "\n",
    "## RNN's\n",
    "\n",
    "LSTMS, GRU's\n",
    "\n",
    "Pretrained Embeddings (with tensorboard visualization)\n",
    "\n",
    "## RL\n",
    "\n",
    "creating a custom environment by subclassing, theres a TF example in the HOML books RL notebook. \n",
    "\n",
    "ENvironment wrappers.\n",
    "\n",
    "## Device management and parallelism\n",
    "\n",
    "code for inter-op and intra\n",
    "\n",
    "model and data parallelism in current practice\n",
    "\n",
    "Distribution Strategies API.\n",
    "\n",
    "---\n",
    "\n",
    "# General CS\n",
    "\n",
    "https://www.coursera.org/learn/build-a-computer\n",
    "\n",
    "that one cs book\n",
    "\n",
    "pair programming with PaLM\n",
    "https://www.deeplearning.ai/short-courses/pair-programming-llm/?_hsmi=275906548&_hsenc=p2ANqtz-9i2MLABiEnqS7gWtaJk0zP1PlAUhqMgYNyFQHPiIRnyp7RDPu2nhb65z3rm6CskK58lhFlbYgpw6HqVd6OVcZ5N1Rtug\n",
    "\n",
    "multi-threading on cpu cores \n",
    "\n",
    "CUDA for hardward acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572b913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e014d07",
   "metadata": {},
   "source": [
    "# practical:\n",
    "\n",
    "\n",
    "## ML \n",
    "\n",
    "---\n",
    "\n",
    "Visualizations are nice and can help if you know what to look for, otherwise focus more on better feature engineering.\n",
    "\n",
    "**quick and dirty baseline:** \n",
    "\n",
    "desribe the data -> imput all missing values with frequent -> one hot encode cat -> run through tabular regression or classifcation custom class -> make submission of basline loss.\n",
    "\n",
    "Then see ML book for project flow ideas and think about how to interpet the data.\n",
    "\n",
    "just use autoML for really fast projects. Seems to be super easy and give really good outcomes.\n",
    "\n",
    "AutoML with AutoGluon \n",
    "\n",
    "AutoML with PyCaret https://pycaret.org/\n",
    "\n",
    "AutoML with H2O \n",
    "\n",
    "---\n",
    "\n",
    "Gradient boosting works well when there is not a lot of data. \n",
    "\n",
    "when doing classification taking the average probability of the output of a few different good classifiers yields better results.\n",
    "\n",
    "## Deeplearning\n",
    "\n",
    "### RL\n",
    "to avoid credit assignment problems remember to interperse smaller rewards if rewards are far between actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e7256a",
   "metadata": {},
   "source": [
    "# Helpful videos \n",
    "\n",
    "take with grain of salt\n",
    "\n",
    "## RNNs\n",
    "https://www.youtube.com/watch?v=L8HKweZIOmg \n",
    "\n",
    "## Transformers\n",
    "\n",
    "https://www.youtube.com/watch?v=1biZfFLPRSY positional embeddings\n",
    "\n",
    "https://www.youtube.com/watch?v=zxQyTK8quyY transformer achitecture \n",
    "\n",
    "https://www.youtube.com/watch?v=A1eUVxscNq8 Multiheaded attention\n",
    "\n",
    "https://www.youtube.com/watch?v=mMa2PmYJlCo Multiheaded attention\n",
    "\n",
    "https://www.youtube.com/watch?v=1IKrHh2X0F0 scaled dot product attention\n",
    "\n",
    "https://www.youtube.com/watch?v=0PjHri8tc1c self and dot product attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a06b0",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# Helpful sites\n",
    "\n",
    "## General\n",
    "https://asmen.icopy.site/awesome/awesome-machine-learning/#reinforcement-learning_1\n",
    "\n",
    "## RNN\n",
    "\n",
    "https://github.com/bentrevett/pytorch-seq2seq/tree/rewrite\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb935ac",
   "metadata": {},
   "source": [
    "## Random extra:\n",
    "\n",
    "https://web.archive.org/web/20230518101138/https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/slutsky.html\n",
    "\n",
    "https://online.stat.psu.edu/stat414/book/export/html/676\n",
    "\n",
    "https://www.sicpdistilled.com/\n",
    "\n",
    "https://math.stackexchange.com/questions/814956/how-fast-does-this-markov-chain-converge\n",
    "\n",
    "https://www.mathsisfun.com/algebra/eigenvalue.html\n",
    "\n",
    "https://awesome-docker.netlify.app/ docker tutorials\n",
    "\n",
    "https://www.udemy.com/course/real-world-devops-project-from-start-to-finish/ devops course (paid)\n",
    "\n",
    "https://sites.research.google/trc/about/ apply for free TPU's as a researcher.\n",
    "\n",
    "https://docs.python.org/3/howto/unicode.html unicode tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc980819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
